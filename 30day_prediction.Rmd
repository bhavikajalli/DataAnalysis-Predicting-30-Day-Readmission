---
title: "LHS 610: Final Project"
output: html_notebook
author: "Bhavika Jalli"
---

Loading the dataset and cleaning

```{r}
library(tidyverse)
data = read_csv("data/diabetic_data_initial.csv")
head(data)
#table(data$readmitted)

```



```{r}
#Cleaning the data and selecting and Recoding the columns for easier use

data_cleaned = data %>% 
  select(encounter_id,race,A1Cresult,num_lab_procedures,number_emergency,number_inpatient, age,gender, admission_type_id, time_in_hospital, num_medications, diag_1, diag_2,diag_3,change,readmitted)


data_cleaned$admission_type_id <- recode(data_cleaned$admission_type_id,`1`= 'Emergency',`2`= 'Urgent',`3` = 'Elective',`4` = 'Newborn',`5` = 'Not Available',`6` = 'NULL',`7` = 'Trauma Center',`8` = 'Not Mapped')

data_cleaned = data_cleaned %>% filter(admission_type_id != 'Not Available',admission_type_id != 'NULL',admission_type_id !='Not Mapped')

```

Checking for missing data
```{r}
#Plot the Missing data
#data_cleaned[data_cleaned == '?'] = NA
#library(Amelia)
#missmap(data_cleaned, main = "Missing values vs observed")

data_cleaned = data_cleaned %>% filter(race != '?',diag_2 != '?',diag_3 != '?')

#summary(data_cleaned)
```

Using the Medical codes to find the Diagnosis
```{r}
#Creating columns called Diagnosis 1,2 and 3 based on the the diagnosis codes.
data_cleaned = data_cleaned %>%
  mutate(Diagnosis_1 = case_when((diag_1 >= 460 & diag_1 <= 519) | diag_1 == 786 ~ 'Respiratory_Disease', 
                                 (diag_1 >= 390 & diag_1 <= 459) | diag_1 == 785 ~ 'Circulatory_Disease',
                                 (diag_1 >= 520 & diag_1 <= 579) | diag_1 == 787 ~ "Digestive_Disease",
                                 diag_1 >= 800 & diag_1 <= 999 ~ "Injury&Poisoning",
                                 diag_1 >= 710 & diag_1 <= 739 ~ "Musculoskeletal",
                                 (diag_1 >= 580 & diag_1 <= 629) | diag_1 == 788 ~ "GenetoUrinary",
                                 diag_1 >= 140 & diag_1 <= 239 ~ "Neoplasms",
                                 startsWith(diag_1,'250') ~ "Diabetes",
                                 TRUE ~ 'Others'))

data_cleaned = data_cleaned %>%
  mutate(Diagnosis_2 = case_when((diag_2 >= 460 & diag_2 <= 519) | diag_2 == 786 ~ 'Respiratory_Disease', 
                                 (diag_2 >= 390 & diag_2 <= 459) | diag_2 == 785 ~ 'Circulatory_Disease',
                                 (diag_2 >= 520 & diag_2 <= 579) | diag_2 == 787 ~ "Digestive_Disease",
                                 diag_2 >= 800 & diag_2 <= 999 ~ "Injury&Poisoning",
                                 diag_2 >= 710 & diag_2 <= 739 ~ "Musculoskeletal",
                                 (diag_2 >= 580 & diag_2 <= 629) | diag_2 == 788 ~ "GenetoUrinary",
                                 diag_2 >= 140 & diag_2 <= 239 ~ "Neoplasms",
                                 startsWith(diag_2,'250') ~ "Diabetes",
                                 TRUE ~ 'Others'))
data_cleaned = data_cleaned %>%
  mutate(Diagnosis_3 = case_when((diag_3 >= 460 & diag_3 <= 519) | diag_3 == 786 ~ 'Respiratory_Disease', 
                                 (diag_3 >= 390 & diag_3 <= 459) | diag_3 == 785 ~ 'Circulatory_Disease',
                                 (diag_3 >= 520 & diag_3 <= 579) | diag_3 == 787 ~ "Digestive_Disease",
                                 diag_3 >= 800 & diag_3 <= 999 ~ "Injury&Poisoning",
                                 diag_3 >= 710 & diag_3 <= 739 ~ "Musculoskeletal",
                                 (diag_3 >= 580 & diag_3 <= 629) | diag_3 == 788 ~ "GenetoUrinary",
                                 diag_3 >= 140 & diag_3 <= 239 ~ "Neoplasms",
                                 startsWith(diag_3,'250') ~ "Diabetes",
                                 TRUE ~ 'Others'))
```



```{r}
data_cleaned = data_cleaned %>% select(-diag_1,-diag_2,-diag_3)
data_cleaned = data_cleaned %>% filter(readmitted != '>30')

ggplot(data= data_cleaned)+geom_bar(aes(x=readmitted,fill = readmitted),show.legend = TRUE)+labs(title = "Distribution of readmitted Patients")

#The data is unbalanced. 

ggplot(data= data_cleaned)+geom_boxplot(aes(x=readmitted,y = number_inpatient,fill = readmitted),show.legend = TRUE,alpha = 0.1)+labs(title = "Readmitted Patients Vs Number of Inpatient Visits")

```
The correlation Plot:
```{r}
#The correlation plot
library(polycor)
x = hetcor(data_cleaned,use="pairwise.complete.obs")
corrplot(x[["correlations"]],method = color)
```

Machine Learning Tasks:

```{r}
library(mlr)
#Creating dummy variables for the columns race,A1Cresult,age,gender,admission type and Diagnosis
x = data_cleaned
labels = data_cleaned %>% select(readmitted)
data_cleaned = data_cleaned %>% select(-readmitted)

data_cleaned = data_cleaned %>% mutate_if(is.character, as.factor)

data_dummy = data_cleaned %>% select(encounter_id:change, Diagnosis_1:Diagnosis_3) %>% createDummyFeatures()
data_dummy$readmitted = x$readmitted

data_cleaned = data_dummy
data_cleaned = data_cleaned %>% mutate_if(is.character, as.factor)

data_cleaned$readmitted = recode(data_cleaned$readmitted,'<30' = '1','NO' = '0')
str(data_cleaned)
#The structure of the data tells us more about the factors and columns

```

The XGBoost Modell:

```{r}
#The XGBoost model with Hyper parameter tuning with 5 fold Cross Validation
set.seed(1000)
train_data = data_cleaned %>% sample_frac(0.66)

test_data = setdiff(data_cleaned,train_data)

# remove the id (not important for prediction)
train_data = train_data %>% select(-encounter_id)
test_data = test_data %>% select(-encounter_id)

#train_data = train_data %>% mutate_if(is.factor, as.character)

train_task = makeClassifTask(data = train_data,
                              target = 'readmitted',positive = '1')
#XGBOOST
getParamSet("classif.xgboost")
xg_set <- makeLearner("classif.xgboost", predict.type = "prob")

#define parameters for tuning
xg_ps <- makeParamSet(
makeIntegerParam("nrounds",lower=200,upper=600),
makeIntegerParam("max_depth",lower=3,upper=20),
makeNumericParam("lambda",lower=0.55,upper=0.60),
makeNumericParam("eta", lower = 0.001, upper = 0.5),
makeNumericParam("subsample", lower = 0.10, upper = 0.80),
makeNumericParam("min_child_weight",lower=1,upper=5),
makeNumericParam("colsample_bytree",lower = 0.2,upper = 0.8)
)

#define search function
rancontrol <- makeTuneControlRandom(maxit = 100L) #do 100 iterations

#3 fold cross validation
set_cv <- makeResampleDesc("CV",iters = 3L)

#tune parameters
xg_tune <- tuneParams(learner = xg_set, task = train_task, resampling = set_cv,measures = auc,par.set = xg_ps, control = rancontrol)

#set parameters
xg_new <- setHyperPars(learner = xg_set, par.vals = xg_tune$x)

#train model
xgmodel <- train(xg_set, train_task)

#test model
predictions <- predict(xgmodel, newdata = test_data )
calculateConfusionMatrix(predictions)
performance(predictions,measures = list(acc,auc,ppv,tpr))
```

The RandomForrest Model:

```{r}
# The RandomForrest Model
set.seed(1000)

#Splitting the Data
train_data = data_cleaned %>% sample_frac(0.66)
test_data = setdiff(data_cleaned,train_data)

# remove the id (not important for prediction)
train_data = train_data %>% select(-encounter_id)
test_data = test_data %>% select(-encounter_id)

#Random Forest
train_task = makeClassifTask(data = train_data,
                              target = 'readmitted',positive = '1')

#train_task <- normalizeFeatures(train_task,method = "standardize")


rf = makeLearner("classif.randomForest", predict.type = "prob",par.vals = list(mtry = 10))
rf$par.vals <- list(importance = TRUE)
rf_param <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 3, upper = 10),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
rancontrol <- makeTuneControlRandom(maxit = 50L)
set_cv <- makeResampleDesc("CV",iters = 5L)
rf_tune <- tuneParams(learner = rf, resampling = set_cv, task = train_task, par.set = rf_param, control = rancontrol, measures = auc)

rf <- setHyperPars(rf, par.vals = list(ntree=500,mtry = 4,nodesize = 12))
rforest <- train(rf, train_task)
predictions = predict(rforest, newdata = test_data)
calculateConfusionMatrix(predictions)
performance(predictions,measures = list(acc,auc,ppv,tpr))
```

RandoForrest for the balanced data:

```{r}
##Equal Data
X = data_cleaned[ sample( which( data_cleaned$readmitted == "0" ) , 9980 ) , ]
Y = data_cleaned[ sample( which( data_cleaned$readmitted == "1" ) , 9980 ) , ]
data_total = full_join(X,Y)


set.seed(1000)

train_data = data_total %>% sample_frac(0.66)
test_data = setdiff(data_total,train_data)

# remove the id (not important for prediction)
train_data = train_data %>% select(-encounter_id)
test_data = test_data %>% select(-encounter_id)

total = data_total %>% select(-encounter_id)
train_task = makeClassifTask(data = train_data,
                              target = 'readmitted',positive = '1')

rf = makeLearner("classif.randomForest", predict.type = "prob",hidden = 3, threshold = 0.01,rep = 10)
top_task <- filterFeatures(train_task, method = "rf.importance", abs = 6)
rf_ps <- makeParamSet(
makeIntegerParam("ntree",lower = 1000, upper = 2000),
makeIntegerParam("mtry", lower = 3, upper = 10),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)

#define search function
rancontrol <- makeTuneControlRandom(maxit = 10L) #do 100 iterations

#3 fold cross validation
set_cv <- makeResampleDesc("CV",iters = 3L)

#tune parameters
rf_tune <- tuneParams(learner = rf, task = train_task, resampling = set_cv,measures = auc,par.set = rf_ps, control = rancontrol)

rforest <- train(rf, train_task)
predictions = predict(rforest, newdata = test_data)
#pred2 = setThreshold(predictions, 0.355)
calculateConfusionMatrix(predictions)
performance(predictions,measures = list(acc,auc,ppv,tpr))

```
